---
title: "Homework2"
author: "Yunjia Liu"
date: "2024-09-30"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
```

## Problem1

### 1. Process the data
Input data and do using clean_names() to clean the data. Using select() to choose the required columns. Specially when treating all the route1~route11, I use the starts_with() function to simplify the process. And then I use mutate to convert 'entry' from character to logical variable.
```{r}
nyctransit_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |>
  janitor::clean_names() |>
  select(line, station_name, station_latitude, station_longitude, starts_with("route"), entry, vending, entrance_type, ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

head(nyctransit_df)

num_rows = nrow(nyctransit_df)
num_cols = ncol(nyctransit_df)
cat("The dataset has", num_rows, "rows and", num_cols, "columns.\n")
```

### 2. Summary:
2.1. Variables in the data:
  - line: The subway line the station belongs to.
  - station_name: Name of the station.
  - station_latitude / station_longitude: Geographic coordinates of the station.
  - routes1~routes11: Subway routes served by the station.
  - entry: whether the entrance allows entry.
  - vending: Whether the entrance has a vending machine.
  - entrance_type: Type of entrance 
  - ada: whether the entrance is ADA compliant.

2.2. Cleaning steps:
After using clean_names() to rename the variables, I retained the relevant columns, converted the entry column to a logical variable, and inspected the data for consistency. The dataset contains 1868 rows and 19 columns. As far as I am concerned, the data is relatively neat because each row corresponds to an entry and each variable has a different value.


### 3. Quesiton
3.1 Number of distinct stations.
```{r}
distinct_stations = nyctransit_df |>
  distinct(line, station_name)

num_distinct_stations <- nrow(distinct_stations)
cat("Number of distinct stations: ", num_distinct_stations, "\n")
```

3.2 Number of stations that are ADA compliant.
```{r}
ada_compliant = nyctransit_df |>
  filter(ada == "TRUE") |>
  distinct(line, station_name)

num_ada_compliant = nrow(ada_compliant)
cat("Number of ADA compliant stations: ", num_ada_compliant, "\n")
```

3.3 Proportion of entrances without vending allow entrance.
```{r}
no_vending_entry = nyctransit_df |>
  filter(vending == "NO") |>
  summarise(proportion_entry = mean(entry))

cat("Proportion of entrances without vending allow entrance: ", no_vending_entry$proportion_entry, "\n")
```

3.4 Question4

Reformat the dataset. 
Using mutate(across(...)) to convert all route columns to characters so that all route1~route11 columns are characters.
Using pivot_longer to combine all route1~route11 columns into a single "route_name" column and make the original column as "route_number".(I am bit confuse about the description)
```{r}
# Use pivot_longer to combine all route1~route11 columns into a single "route" column

transit_reformat = nyctransit_df |>
  mutate(across(starts_with("route"), as.character)) |>
   pivot_longer(route1:route11, names_to = "route_number", values_to = "route_name", values_drop_na = TRUE)
head(transit_reformat)
```

Filter the dataset to find stations that serve the A train and count the number of distinct stations serving the A train
```{r}
a_train_stations = transit_reformat |>
  filter(route_name == "A") |>
  distinct(line, station_name)

num_a_train_stations = nrow(a_train_stations)
cat("Number of distinct stations serving the A train: ", num_a_train_stations, "\n")
```

Find the number of A train stations that are ADA compliant and count the number of ADA compliant stations serving the A train.
```{r}
ada_compliant_a_train = transit_reformat |>
  filter(route_name == "A") |>
  filter(ada == "TRUE") |>
  distinct(line, station_name)

num_ada_compliant_a_train = nrow(ada_compliant_a_train)
cat("Number of ADA compliant stations serving the A train: ", num_ada_compliant_a_train, "\n")
```


## Problem2

### 1. Process the data
1.1 Input the data in 'Mr. Trash Wheel' Sheet from '202409_Trash_Wheel_Collection_Data.xlsx' and point to the specific sheet 'Mr. Trash Wheel'. I am skipping the first row because the it contains a picture and logo. Also, I use only col form A to N because the column O and column P are completely emtpy column. Then, use janitor::clean_names() to rename the variables and NA of 'dumpster' variable is dropped. According the requirement, I rounded the 'sports_balls' variable and convert it to integar and change the type of 'year' variables from character to integer. Finally, I use mutate() function to create a new column in order to record the sourcing ('Mr. Trash Wheel') of the dataset.
```{r input and clean data from Mr. Trash Wheel}
mr_trash_wheel =
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Mr. Trash Wheel", range = cell_cols("A:N"), skip = 1, na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  drop_na(dumpster)|>
  mutate(
    sports_balls = as.integer(round(sports_balls, 0)), 
    trash_wheel = "Mr. Trash Wheel",
    year = as.integer(year))
```


1.2 Read and clean Professor Trash Wheel sheet & Gwynnda's data sheet. They are both skipping the first row because the sheet starts with a logo in the first row. Combine all three separate datasets together.
```{r input and clean data from Prof and Gwynnda}
prof_trash_wheel = read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", 
                                sheet = "Professor Trash Wheel", 
                                skip = 1,  # Adjust based on where data starts
                                col_names = TRUE,
                                na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  drop_na(dumpster)|>
  mutate(
    year = as.integer(year),
    trash_wheel = "Professor Trash Wheel"
  )

gwynnda_trash_wheel = read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", 
                                   sheet = "Gwynnda Trash Wheel", 
                                   skip = 1,
                                   col_names = TRUE,
                                  na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  drop_na(dumpster)|>
  mutate(
    year = as.integer(year),
    trash_wheel = "Gwynnda Trash Wheel"
  )

# Combine datasets into one tidy dataset
combined_trash_data <- bind_rows(mr_trash_wheel, prof_trash_wheel, gwynnda_trash_wheel)
```


### 2. Answer to the question: 
2.1.total weight of trash collected by Professor Trash Wheel
```{r total weight of trash}
total_weight_prof = combined_trash_data |>
  filter(trash_wheel == "Professor Trash Wheel") |>
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))
```

2.2. total number of cigarette butts collected by Gwynnda in June 2022:
```{r total number of cigarette butts}
total_cig_butts_gwynnda = combined_trash_data |>
  filter(trash_wheel == "Gwynnda Trash Wheel" & month(date) == 6 & year == 2022) |>
  summarise(total_cig_butts = sum(`cigarette_butts`, na.rm = TRUE))

```

### 3. Summary:
```{r summary of answers to questions in problem2}
n_observations = nrow(combined_trash_data)
total_weight_prof = total_weight_prof$total_weight
total_cig_butts_gwynnda = total_cig_butts_gwynnda$total_cig_butts

cat("The combined dataset includes", n_observations, "observations from three Trash Wheels: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda. 
Key variables in the dataset include `weight_tons` (the weight of trash collected in tons) and `cigarette_butts`. 
For example, Professor Trash Wheel has collected a total of", total_weight_prof, "tons of trash. 
In June 2022, Gwynnda collected a total of", total_cig_butts_gwynnda, "cigarette butts.")
```


## Problem3
3.1 Input three datasets. I split the 'baker_name' variable in the 'bakers.csv' because it is the full name of the baker, however the rest two datasets are using the first name of the baker.
```{r problem3_input, message=FALSE}

bakers_df = 
  read_csv("./data/gbb_datasets/bakers.csv") |>
  janitor::clean_names() |>
  mutate(
    baker = str_split(baker_name, " ", simplify = T)[,1]
  )

bakes_df = 
  read_csv("./data/gbb_datasets/bakes.csv") |>
  janitor::clean_names()

results_df =  
  read_csv("./data/gbb_datasets/results.csv",skip = 2) |>
  janitor::clean_names() |>
  drop_na(baker)

```

3.2 Using anti_join() to check for completeness and correctness across datasets 
```{r assessment for completeness and correctness}
unmatched_bakes_bakers <- bakes_df%>%
  anti_join(bakers_df, by = c("baker", "series"))
print(unmatched_bakes_bakers)

unmatched_results_bakers <- results_df %>%
  anti_join(bakers_df, by = c("baker", "series"))
print(unmatched_results_bakers)

unmatched_bakes_results <- bakes_df %>%
  anti_join(results_df, by = c("baker", "series", "episode"))
print(unmatched_bakes_results)
```

3.3 From the results, it is obvious that the '\jo\' ('baker' variables) in bakes.csv and baker.csv is referring to the 'Joanne' ('baker' variables) in results.csv. The following code is for this correction, renaming the 'Joanne' and '\"Jo\"'to 'Jo' in the results_df and bakes_df respectively.
```{r recleaning}

results_df_clean = 
  results_df |>
  mutate(baker = recode(baker, 'Joanne' = 'Jo'))

bakes_df_clean =
  bakes_df |>
  mutate(baker = recode(baker,'\"Jo\"' = 'Jo'))
```

3.4 recheck the completeness and correctness across datasets after re-cleaning.
```{r re_asessement}
unmatched_bakes_bakers_clean <- bakes_df_clean%>%
  anti_join(bakers_df, by = c("baker", "series"))
print(unmatched_bakes_bakers_clean)

unmatched_results_bakers_clean <- results_df_clean %>%
  anti_join(bakers_df, by = c("baker", "series"))
print(unmatched_results_bakers_clean)

unmatched_bakes_results_clean <- bakes_df_clean %>%
  anti_join(results_df_clean, by = c("baker", "series", "episode"))
print(unmatched_bakes_results_clean)
```

3.5 Combine the bakes and corresponding results.

I first combine the bakers_df and bake_df_clean using baker's first name and series they appear in so that the first name can be specific enough to point to one person. Then, I focus on the combine of the results_df. Since the bakes and results of same person have difference in each episode, so I use three variables in linking the dataset.
```{r combine the dataset}
final_data =
  bakers_df |>
  left_join(bakes_df_clean, by = c("baker", "series")) |>
  left_join(results_df_clean, by = c("baker", "series","episode"))
```

3.6 Reformat some details.

Since we have baker's full name in baker.csv file, I am dropping baker's first name coloumn('baker' variable) from bake.csv and results.csv. Also,the columns are ordered in a way easy for users to look up. It starts with the series and episode, then comes with the biography of the baker and their signature bake, and finally is their technical score given by the judge and result. I have re-coded the result into a readable way according to the caption in results.csv file.

```{r last reformat of the combined dataset}
final_data_clean = 
  final_data |>
  select(series, episode, baker_name, baker_age, baker_occupation, hometown, signature_bake, show_stopper, technical, result) |>
  mutate(
    result = recode(result, "IN" = "stayed in", 'OUT' = 'Eliminated','STAR BAKER' = 'Star Baker', 'WINNER' = 'Series Winner', 'Runner-up' = 'Series Runner up', 'WD' = 'withdrew')
  )


dim(final_data_clean)
str(final_data_clean)
head(final_data)
summary(final_data_clean)

write_csv(final_data_clean, "./data/final_bake_off_data.csv")

```

Discuss about the final_data_clean:

The dataset has 10 variables covering the exact series and episode show that each baker was in,  biography of the bakers, their bake and their result. In total, we have 573 rated bakes and their bakers. All the steps I used to clean and combine the data are written above. Please refer to 3.1~3.6 for the exact steps.

3.7 Create a table of Star Bakers for Seasons 5-10
```{r Star Bakers for Seasons 5-10}
star_bakers = final_data_clean |>
  filter(series %in% 5:10 & (result == "Star Baker"|result == "Series Winner")) |>
  select(series, episode, baker_name, signature_bake, technical, result) |>
  arrange(series, episode)

print(star_bakers,n=Inf)

```

Conclusion: 
- the dataset only supports series 5 to 8(since series 9 and 10 dosen't have the exact result for each baker)
-We assume that there is a strong relationship between the number of times each baker gets a STAR BAKER and getting WINNER OF THE SEASON. That is to say, the more 'star bakers' a baker gets, the more likely he or she is to be the winner of the season. Therefore, we can comment the results on this base. For example, if someone got never/less times of star bakers win the season, we will call it a surprise.
- In season5, Nancy Birtwhistle's winning is a surprise considering Richard Burr had won 4 'star bakers' over the season while Nancy only got one.
- In season6 and season 7 , Nadiya Hussain and Candice Brown are absolutely predictable winners.They had won 'star bakers' throughout the season 3 times and twice, respectively. The number of times winning 'star bakers' are way over others.
- In season 8, Sophie Faldo's winning of the series is quite a surprice since she had only won one 'Star Baker' during the whole season.

3.8 Import, clean, and analyze viewers.csv
```{r Import viewers_df, message=FALSE}
viewers_df =
  read_csv("./data/gbb_datasets/viewers.csv") |>
  janitor::clean_names()
```


```{r clean and analyze viewers_df}
# Show first 10 rows
head(viewers_df,10)

# Calculate average viewership for Seasons 1 and 5
avg_viewership_season_1 = viewers_df |>
  summarise(avg_viewership = mean(series_1, na.rm = TRUE))
avg_viewership_season_5 = viewers_df |>
  summarise(avg_viewership = mean(series_5, na.rm = TRUE))

cat("the average viewership in Season 1:",avg_viewership_season_1$avg_viewership,"the average viewership in Season 5:",avg_viewership_season_5$avg_viewership,"")

```

